# 使用指南

## 快速开始

### 1. 安装依赖

```bash
# 安装后端依赖
cd backend
npm install

# 安装前端依赖
cd ../frontend
npm install
```

### 2. 配置环境变量

在 `backend` 目录下创建 `.env` 文件：

```bash
cd backend
cp .env.example .env
```

编辑 `.env` 文件，根据需要调整配置。

### 3. 启动服务

```bash
# 启动后端服务（在 backend 目录）
npm run dev

# 启动前端服务（在 frontend 目录，新开终端）
cd ../frontend
npm run dev
```

访问 http://localhost:5173 开始使用。

## 使用流程

### 步骤一：配置模型

1. 访问"模型配置"页面
2. 点击"添加模型"按钮
3. 配置至少一个翻译模型（阶段一）

**推荐配置：**
- **翻译模型**（必需）：至少配置 2-3 个不同的模型来进行初始翻译
- **审核模型**（可选）：配置 1-2 个模型来评价翻译质量
- **综合模型**（可选）：配置 1-2 个模型来生成最终译文
- **嵌入模型**（可选）：如果需要使用知识库功能，需要配置嵌入模型

**模型配置示例：**

#### OpenAI GPT-4
- 名称：GPT-4 翻译
- 阶段：翻译模型
- API端点：`https://api.openai.com/v1/chat/completions`
- API Key：`sk-...`
- 模型ID：`gpt-4`
- 系统提示词：
  ```
  你是一位专业的法律翻译专家，精通法律英语和中文。
  请准确翻译法律文本，保持专业术语的准确性，确保译文流畅易懂。
  ```
- 温度：0.3

#### Claude
- 名称：Claude 审核
- 阶段：审核模型
- API端点：`https://api.anthropic.com/v1/messages`
- API Key：`sk-ant-...`
- 模型ID：`claude-3-opus-20240229`
- 系统提示词：
  ```
  你是一位资深的法律翻译审核专家。
  请从准确性、专业性、流畅性等方面评价翻译质量，
  并提供具体的改进建议。
  ```

### 步骤二：配置知识库（可选）

如果需要使用知识库增强翻译：

1. 首先在"模型配置"中添加一个嵌入模型
2. 访问"知识库"页面
3. 点击"上传文档"
4. 选择嵌入模型并上传法律词典等参考文档

**支持的文件格式：**
- TXT - 纯文本文件
- PDF - PDF文档
- DOCX - Word文档
- MD - Markdown文件

**文件大小限制：** 10MB

### 步骤三：开始翻译

1. 在主页输入法律英语文本
2. （可选）勾选"使用知识库增强"并选择知识库
3. 点击"开始翻译"
4. 系统将自动执行三阶段工作流：
   - **阶段一**：多个翻译模型并行翻译
   - **阶段二**：审核模型评价翻译质量
   - **阶段三**：综合模型生成最终译文
5. 查看各阶段结果和最终译文

### 步骤四：查看历史

访问"历史记录"页面可以查看所有翻译历史，点击任意记录可查看详细信息。

## 三阶段工作流详解

### 阶段一：初始翻译
- 所有启用的翻译模型并行工作
- 每个模型独立生成译文
- 如果启用知识库，会自动检索相关内容辅助翻译

### 阶段二：审核评价
- 审核模型评价每个翻译结果
- 从准确性、专业性、流畅性等维度评分
- 提供具体的改进建议

### 阶段三：综合翻译
- 综合前两阶段所有信息
- 吸收各模型优点
- 生成最优的最终译文

## 常见问题

### Q: 如何测试模型配置是否正确？
A: 在模型配置页面，点击模型卡片右侧的"测试"按钮。

### Q: 可以只使用翻译模型吗？
A: 可以。审核和综合阶段是可选的。如果只配置翻译模型，系统只会执行阶段一。

### Q: 知识库如何工作？
A: 系统会将文档切分成小块并生成向量嵌入。翻译时，会检索与输入文本最相关的内容，提供给模型作为参考。

### Q: 为什么翻译很慢？
A: 翻译速度取决于：
- 配置的模型数量（模型越多，耗时越长）
- API响应速度
- 文本长度
- 是否使用知识库

### Q: 如何提高翻译质量？
A: 建议：
1. 配置多个不同的翻译模型
2. 使用专业的审核模型
3. 上传相关的法律词典到知识库
4. 优化系统提示词
5. 适当调整温度参数（0.2-0.5 适合翻译任务）

### Q: 可以使用本地模型吗？
A: 可以。只要提供兼容 OpenAI API 格式的端点即可，例如：
- Ollama: `http://localhost:11434/v1/chat/completions`
- LocalAI: `http://localhost:8080/v1/chat/completions`
- vLLM: `http://localhost:8000/v1/chat/completions`

## 高级配置

### 自定义参数

在模型配置中，可以设置：
- **temperature**: 控制输出随机性（0-2，推荐 0.3-0.7）
- **maxTokens**: 最大输出长度
- **topP**: 核采样参数
- **frequencyPenalty**: 频率惩罚
- **presencePenalty**: 存在惩罚
- **customParams**: 自定义参数对象

### 系统提示词模板

#### 翻译模型提示词
```
你是一位专业的法律翻译专家，精通法律英语和中文。

翻译原则：
1. 准确传达法律术语的含义
2. 保持原文的语气和风格
3. 确保译文通顺易懂
4. 必要时保留关键的英文术语并加注中文

请翻译以下法律英语文本：
```

#### 审核模型提示词
```
你是一位资深的法律翻译审核专家。

请从以下维度评价翻译质量：
1. 准确性：是否准确表达了原文的法律含义
2. 专业性：法律术语使用是否恰当
3. 流畅性：译文是否通顺易懂
4. 完整性：是否有遗漏或添加

请给出评分（1-10分）和具体的改进建议。
```

#### 综合模型提示词
```
你是一位经验丰富的法律翻译专家。

请综合考虑以下所有翻译结果和审核意见：
1. 吸收各译文的优点
2. 采纳审核建议
3. 生成一个更准确、专业、流畅的最终译文

只需输出最终译文，不需要额外解释。
```

## 技术支持

如有问题，请查看：
- [GitHub Issues](https://github.com/XimilalaXiang/LLM-Translater/issues)
- [项目文档](./README.md)
